all right hi everyone and welcome back my name is ava and before we dive into lecture two of success 191 which is
0:16
going to be on deep sequence modeling i'll just note that as you probably noticed we're running a little bit late

0:21
so we're going to proceed with the lecture you know in full and in completion and at the time it ends then
0:27

we'll transition to the software lab portion of the course just immediately after at the time that this lecture ends
0:32
and i'll make a note about the structure and how we're going to run the software labs at the end of my lecture

0:38
okay so in alexander's first lecture we learned about really the essentials of neural networks and feed-forward models
0:45

and how to construct them so now we're going to turn our attention to applying neural networks to tasks that involve
0:52
modeling sequences of data and we'll see why these sorts of tasks require a fundamentally different type

0:59
of network architecture from what we've seen so far and to build up to that point we're going to walk through step by step
1:05

building up intuition about why modeling sequences is different and important and
1:11
start back with our fundamentals of feed forward networks to build up to the models we'll introduce in this lecture

1:18
all right so let's dive into it let's first motivate the need for sequence modeling and what we mean in
1:24

terms of sequential data with a super intuitive and simple example so suppose we have this picture of a ball
1:31
and our task is to predict where this ball is going to travel to next now if i don't give you any prior

1:37
information on the ball's history any guess on its next position is just going to be that a random guess
1:44

but now instead if in addition to the current location of the ball i also gave you some information about its previous
1:51
locations now our problem becomes much easier and i think we can all agree that we have a

1:56
sense of where this ball is going to next and beyond this simple example the fact
2:03

of the matter is that sequential data is all around us for example audio like the waveform of my voice speaking to you can
2:10
be split up into a sequence of sound waves while text can be split up into a

2:17
sequence of characters or a sequence of words and beyond these two examples there are
2:22

many many more cases in which sequential processing may be useful from medical signals like ekgs to stock
2:30
prices to dna sequences and beyond so now that we've gotten a sense of what

2:35
sequential data looks like let's consider applications of sequential modeling in the real world
2:42

in alexander's first lecture we learned about this notion of feed-forward models that operate sort of on this one-to-one
2:49
fixed setting right a single input to a single output and he gave the very simple example of a binary

2:55
classification task predicting whether you as a student will pass or fail this
3:00

class of course we all hope you will pass but in this example there's no real
3:06
component of time or sequence right in contrast with sequence modeling we

3:12
can now handle a vast variety of different types of problems where for example we have a sequence of
3:19

temporal inputs and potentially a sequential output so let's consider one example right
3:25
where we have a natural language processing task where we have a tweet and we want to classify

3:31
the emotion or the sentiment associated with that tweet mapping a sequence of words to a positive or negative label
3:41

we can also have a case where our input initially may not have
3:47
a time dimension so for example we have this image of a baseball player throwing a ball but instead the output that we

3:54
want to generate has a temporal or sequential component where we now want to caption that image with some
4:01

associated text and finally we can have a final case where we have a sequential
4:07
input and we want to map it to a sequential output for example in the case of translating text from one

4:13
language to another and so sometimes it can be really challenging to kind of wrap your head
4:20

around and get the idea about how we can add a new temporal dimension to our
4:25
models and so to achieve this understanding what i want to do is really start from the fundamentals and revisit the concept

4:33
of the perceptron that alexander introduced and go step by step from that foundation to develop an understanding
4:40

of what changes we need to make to be able to handle sequential data so let's recall the architecture and the
4:48
the diagram of the perceptron which we studied in the first lecture we defined a set of inputs

4:54
and we have these weights that are associated with connecting those inputs to an
4:59

internal node and we can apply those weights apply a non-linearity and get this output
5:05
and we can extend this now to a layer of individual neurons a layer of

5:10
perceptrons to yield a multi-dimensional output and in this example we have a single
5:16

layer of perceptrons shown in green taking three inputs shown in blue
5:21
predicting four outputs in purple but does this notion does this have a notion of time or of

5:27
sequence not yet let's simplify that diagram right what
5:33

i've done here is just i've collapsed that layer of those four perceptrons into the single green box
5:40
and i've collapsed those nodes of the input and the output into these single circles that are

5:45
represented as vectors so our inputs x are some vectors of a
5:51

length m and our outputs are vectors of another length n
5:56
still here what we're considering is an input at a specific time denoted by t nothing different from what

6:03
we saw in the first lecture and we're passing it through a feed-forward model to get some output
6:10

what we could do is we could have fed in a sequence to this model by simply
6:16
applying the same model that same series of operations over and over again

6:22
once for each time step in our sequence and this is how we can handle these
6:27

individual inputs which occur at individual time steps so first let's just rotate the same diagram i've taken
6:34
it from a horizontal view to a vertical view we have this input vector at some time sub t we feed it into our network

6:40
get our output and since we're interested in sequential data let's assume we don't just have a single
6:47

time step we now have multiple individual time steps starting from t equals zero our first time step in our
6:54
sequence and extending forward right again now we're treating the individual

7:01
time steps as isolated time steps right we don't yet have a notion of the
7:06

relationship between time step zero and time step one time step two and so on
7:11
and so forth and what we know from the first lecture is that our output vector at a

7:17
particular time step is just going to be a function of the input at that time step
7:23

what could be the issue here right well we have this transformation yet
7:29
but this is inherently sequential data and it's probably in a sequence for some

7:34
important reason and we don't yet have any sort of interdependence or
7:40

notion of interconnectedness across time steps here
7:45
and so if we consider the output at our last time step right

7:50
the fundamental point is that that output is related to the inputs at the
7:56

previous time steps how can we capture this interdependence
8:02
what we need is a way to relate the network's computations at a particular time step to its prior history and its

8:10
memory of the computations from those prior time steps passing information forward propagating
8:15

it through time and what we consider doing is actually linking the information and the
8:21
computation of the network at different time steps to each other via what we

8:27
call a recurrence relation and specifically the way we do this in
8:32

neural recurrent recurrent models is by having what we call an internal memory or a state which
8:40
we're going to denote as h of t and this value h of t is maintained time

8:46
set to time step and it can be passed forward across time and the idea and the intuition here is
8:53

we want the state to try to capture some notion of memory and what this means for the network's
8:59
computation its output is that now our output is dependent not only on the

9:05
input at a particular time step but also this notion of the state of the memory
9:11

that's going to be passed forward from the prior time step
9:16
right and so this output just to make this very explicit this output at a

9:22
particular time step t depends both on the input as well as the past memory
9:29

and that past memory is going to capture the prior history of what the what has
9:34
occurred previously in the sequence and because this output y of t is a

9:41
function of both current input past memory what this means is we can define and
9:46

describe these types of neurons in terms of a recurrence relation and so on the right you can see how we
9:53
visualize these individual time steps as sort of being unrolled extended across

9:58
time but we could also depict this same relationship via a cycle which i've
10:03

shown on the left which shows and highlights this concept of a recurrence relation
10:09
all right so hopefully this builds up some intuition about this notion of recurrence and why it can help us in

10:16
sequential modeling tasks and this intuition that we've built up from starting with the feed forward
10:22

model is really the key to the recurrent neural network or rnns and we're going
10:28
to continue to build up from this foundation and build up our understanding of how this recurrence

10:33
relation defines the behavior of an rnn so let's formalize this just a bit more
10:39

right the key idea that i mentioned and i'm going to keep driving home is that the rnn maintains this internal state h
10:46
of t which is going to be updated at each time step as the sequence is processed

10:51
and we do this by applying this recurrence relation at every time step where our cell state
10:58

is now a function yeah our cells ourselves say h of t is now a function of the current input
11:07
x of t as well as the prior state h of t minus 1. and importantly this function is

11:13
parametrized by a set of weights w and this set of weights is what we're
11:18

actually going to be learning through our network over the course of training as the model is being learned
11:25
and as these weights are being updated right and the key point here is that this set of weights w is the same

11:33
across all time steps that are being considered in the sequence and this function that computes this
11:39

hidden state is also the same we can also step through this intuition
11:45
behind the rnn algorithm in sort of pseudo code to get a better sense of how these networks work

11:52
so we can begin by initializing our rnn right what does it take to initialize it
11:58

well first we have to initialize some first hidden state which we're going to do with a vector of zeros
12:04
and we're going to consider a sentence that's going to serve as our input sequence to the model and our task here

12:11
is to try to predict the next word that's going to come at the end of the sentence
12:17

and our recurrence relation is captured by this loop where we're going to iterate through the words in the
12:23
sentence and at each step we're going to feed both the current word

12:29
being considered as well as the previous hidden state into our rnn model and that's going to output a prediction for
12:36

what the likely next word is and also update its internal computation of the hidden state
12:42
and finally our last our token prediction that we're interested in at the end

12:48
is the rnn's output after all the words all the time points in the sequence have been considered
12:53

and that generates our prediction for the likely next word and so that's that hopefully provides
13:01
more intuition about how this rnn algorithm is working and if you notice the internal

13:07
computation of the rnn includes both this internal state output
13:12

as well as ultimately trying to output the prediction that we're interested in our output vector y of t
13:19
so to walk through how this we actually derive this output vector

13:24
let's step through this what we do is given an input our input vector
13:29

we pass that in to compute the rnn's internal state computation
13:35
and breaking this function down what it's doing is just a standard neural net operation

13:41
just like we saw in the first lecture right it consists of multiplication by weight
13:46

matrices right donated as w and in this case we're going to multiply
13:52
both the past hidden state by a weight matrix w as well as the current input x of t by

13:59
another wave matrix and then we're going to add them together and apply a non-linearity
14:06

and you'll notice as i just mentioned right because we have these two inputs to the state update equation we have these two
14:13
independent weight matrices and the final step is to actually

14:19
generate the output for a given time step which we do by taking that internal
14:24

state and simply modifying it following a multiplication by another weight
14:29
matrix and then using this as our generated output and that's it that's how the rnn updates

14:36
its hidden state and also produces an output at a given time step
14:42

so so far right we've seen the rnns depicted largely as having these loops
14:48
that feed back in on themselves and as we as we built up from we can also represent this loop as being

14:55
unrolled across time where effectively starting from the first time step we have this unrolled
15:01

network that we can continue to unroll across time from time step 0 to our n
15:06
time step time sub t and in this diagram let's now formalize things a little bit more we

15:13
can also make the weight matrices that compute that are applied to the input
15:18

very explicit and we can also annotate our diagram with the way matrices that relate the
15:25
prior hidden state to the current hidden state and finally our predictions at

15:30
individual time steps are um are generated by a a separate weight matrix
15:36

matrices okay so as i as i mentioned right the key
15:42
point is that these weight matrices are reused across all of the individual time

15:48
steps now our next step that you may be thinking of is okay this is all great we
15:54

figured out how to update the hidden state we figured out how to generate the output how do we actually train this
15:59
thing right well we'll need a loss because as alexander mentioned the way

16:04
we train neural networks is through this optimization this iterative optimization of a loss function or an objective
16:10

function and as you may may predict right we can generate an individual loss for each of
16:17
these individual time steps according to what the output at that time step is

16:23
and we can generate a total sum loss by taking these time steps and summing them all together
16:30

and when we make a forward pass through our network this is exactly what we do right we generate our output predictions
16:36
and we sum sum the loss uh functions across individual time steps to get the total loss

16:44
now let's walk through let's next walk through an example of how we can implement an rnn from scratch the
16:51

previous code block that i showed you was kind of an intuitive pseudo code example and here now we're going to get
16:57
into things in a little bit more detail and build up the rnn from scratch our rnn is going to be defined as a

17:04
neural network layer and we can build it up by inheriting from the neural network layer class that
17:10

alexander introduced in the first lecture and as before we are going to start by
17:15
initializing our weight matrices and also initializing the hidden state to zero

17:21
our next step which is really the important step is defining the call function which is what actually defines
17:28

the forward pass through our rnn model and within this call function the key
17:33
operations are as follows we first have a update of the hidden state right according to that same

17:40
equation we saw earlier incorporating the previous hidden state incorporating the input
17:46

summing them passing them through a non-linearity we can then compute the output
17:52
transforming the hidden state and finally at each time step we return both the current output and our hidden

17:58
state that's it that's how you can code up an rnn line by line and define the
18:05

forward pass but thankfully tensorflow has very very conveniently
18:10
summarized this already and implemented these types of rnn cells for us in what they wrap into the simple rnn

18:17
layer and you're going to get some practice using this um this class of neural network layer in today's software
18:23

lab all right so to recap right we've we've
18:29
seen how uh we've seen the the function and the computation of rnns by first moving

18:35
from the one-to-one computation of a traditional feed-forward vanilla or
18:40

vanilla neural network excuse me and seeing how that breaks down when considering sequence modeling problems
18:48
and as i mentioned right we can we can apply this idea of sequence modeling and of rnns to many different types of tasks

18:55
for example taking a sequential input and mapping it to one output taking a static input that's not
19:02

resolved over time and generating a sequence of outputs for example a text
19:08
associated with an image or translating a sequence of inputs to a

19:14
sequence of outputs which can be done in machine translation natural language processing and also in
19:20

generation so for example in composing new musical scores entirely
19:26
using recurrent neural network models and this is what you're going to get your hands-on experience with in today's

19:32
software lab and beyond this right you know you all come from a variety of different
19:38

backgrounds and interests and disciplinary domains so i'm sure you can think of a variety of other applications
19:45
where this type of architecture may be very useful

19:51
okay so to wrap up this section right this simple example of rnns kind of motivates a set of concrete design
19:57

criteria that i would like you to keep in mind when thinking about sequence modeling problems
20:03
specifically whatever model we design needs to be able to handle sequences of

20:08
variable length to track long-term dependencies in the data to be able to map something that
20:15

appears very early on in the sequence to something related later on in the sequence
20:20
to be able to preserve and reason and maintain information about order

20:26
and finally to share parameters across the sequence to be able to keep track of
20:31

these dependencies and so most of today's lecture is going to
20:37
focus on recurrent neural networks as a workhorse neural network architecture

20:42
for sequence modeling criteria design problems but we'll also get into a new and
20:49

emerging type of architecture called transformers later on in the lecture which i think you'll find really
20:54
exciting and really interesting as well before we get into that i'd like to

21:00
spend a bit of time thinking about the these design criteria that i enumerated and
21:06

why they're so important in the context of sequence modeling and use that to move forward into some concrete
21:12
applications of rnns and sequence models in general so let's consider a very simple sequence

21:19
modeling problem suppose we have this sentence right this morning i took my cat for a walk
21:26

and our task here is to use some prior information in the sentence to predict
21:32
the next word in the sequence right this morning i took my cat 4a predict the next work walk

21:39
how can we actually go about doing this right i've introduced the intuition and the diagrams and everything about the
21:46

recurrent neural network models but we really haven't started to think about okay how can we even represent language to a neural
21:53
network how can we encode that information so that it can actually be passed in and operated on mathematically

22:00
so that's our first consideration right let's suppose we have a model
22:05

we're inputting a word and we want to use our neural network to predict the next work word
22:12
what are considerations here right remember the neural network all it is is it's just a functional operator

22:18
they execute some functional mathematical operation on an input they can't just take a word as a string or as
22:26

as a as a language as a sequence of language characters passed in as is that's simply not going
22:34
to work right instead we need a way to represent these

22:39
elements these words numerically to be fed in to our neural
22:44

network as a vector or a matrix or an array of numbers such that we can operate on it mathematically and get a
22:51
vector or array of numbers out this is going to work for us so how can we actually encode language

22:58
transform it into this vector representation the solution is this concept of what we
23:04

call an embedding and the idea is that we're going to transform a set of
23:09
indices which effectively are just identifiers for objects into some vector of fixed size

23:17
so let's think through how this embedding operation could work for language data for example for this
23:24

sequence that we've been considering right we want to be able to map any word that
23:29
could appear in our body of language our corpus into a fixed sized vector

23:36
and so the first step to doing this is to think about the vocabulary right what's the overall
23:42

space of unique words in your corpus in your language
23:47
from this vocabulary we can then index by mapping individual words to numerical

23:54
unique indices and then these indices can then be mapped to an embedding which
24:00

is just a fixed length vector one way to do this is by
24:05
taking a vector right that's length is just going to equal the

24:10
total number of unique words in our vocabulary and then we can indicate
24:16

what word that vector corresponds to by making this a sparse vector that's just binary so it's just zeros
24:23
and ones and at the index that corresponds to that word we're going to indicate the identity of that word with

24:30
a one right and so in this example our word is cat and we're going to index it at the second index
24:37

and what this is referred to is a one hot embedding it's a very very popular
24:42
embed choice of embedding which you will encounter across many many different domains

24:48
another option to generating and embedding is to actually use some sort of machine learning model it can be a
24:54

neural network to learn in embedding and so the idea here is from taking an input
25:00
of words that are going to be indexed numerically we can learn an embedding of

25:05
those words in some lower dimensional space and the motivation here is that by
25:11

introducing some sort of machine learning operation we can map the meaning of the words to
25:16
an encoding that is more informative more representative such that

25:22
similar words that are semantically similar in meaning will have similar embeddings
25:28

and this will also get us our fixed length encoding vector and this idea of a learned embedding is
25:34
a super super powerful concept that is very pervasive in modern deep learning today

25:40
and it also motivates a whole nother class of problems called representation learning which is focused on how we can
25:47

take some input and learn use neural networks to learn a meaningful
25:53
meaningful encoding of that of that input for our problem of choice

25:59
okay so going back to our design criteria right we're first going to be able to try to handle variable sequence
26:06

lengths we can consider again this this problem of trying to predict the next word we
26:11
can have a short sequence we can have a longer sequence or an even longer sequence right but the whole point is

26:17
that we need to be able to handle these variable length inputs and feed forward
26:22

networks are simply not able to do this because they have inputs of fixed dimensionality
26:28
but because with rnns we're unrolling across time we're able to handle these variable sequence lengths

26:35
our next our next criteria is that we need to be able to capture and model
26:40

long-term dependencies in the data so you can imagine an example like this where information from early on in the
26:47
sentence is needed to make a accurately make a prediction later on in the sentence and so we need to be able

26:54
to capture this longer term information in our model and finally we need to be able to
27:01

retain some sense of order right that could result in differences
27:06
in the overall contact or meaning of a sentence so in this example these two sentences have the exact same

27:13
words repeated the exact same number of times but the semantic meaning is completely different because the words
27:19

are in different orders and so hopefully this example shows a
27:25
very concrete and common example of sequential data right language

27:31
and motivates how these different design considerations uh play into this general
27:37

problem of sequence modeling and so these points are something that i really like for you to take away from
27:42
this class and keep in mind as you go forward implementing these types of models in practice

27:49
our next step as we as we walk through this lecture on sequence modeling is to actually go through very briefly on the
27:56

algorithm that's used to actually train recurrent neural network models and that algorithm is called backpropagation
28:03
through time and it's very very related to the backpropagation algorithm that alexander

28:08
introduced in the first lecture so if you recall the way we train feedforward models is go from input and make a
28:16

forward pass through the network going from input to output and then back propagate our gradients
28:22
back downwards through the network taking the derivative of the loss with respect to the weights learned by our

28:30
model and then shifting and adjusting the parameters of these weights in order to try to
28:37

minimize the loss over the course of training and as we saw earlier for rnns they have
28:44
a little bit of a different scenario here because our forward pass through the network consists of going forward

28:49
across time computing these individual loss values at the individual time steps and then summing them together
28:57

to back propagate instead of back propagating errors through a single feed forward network now what we have to do
29:04
is back propagate error individually across each time step and then across all the time steps all the way from

29:11
where we currently are in this sequence to the beginning of the sequence and this is the reason why this
29:16

algorithm is called backpropagation through time because as you can see errors flow backwards in time to the
29:23
beginning of our data sequence and so taking a closer look at how these

29:30
gradients flow across this rnn chain what you can see is that between each
29:35

time step we need to perform these individual matrix multiplications right
29:40
which means that computing the gradient that is taking the loss with respect to

29:47
an internal state and the weights of that internal state requires many many matrix in
29:52

multiplications involving this weight matrix as well as repeated gradient
29:58
computation so why might this be problematic well

30:03
if we have many of these weight values or gradient values that are much much much larger than one
30:09

we could have a problem where during training our gradients effectively explode
30:15
and the idea behind this is the gradients are becoming extremely large due to this repeated multiplication

30:22
operation and we can't really do optimization and so a simple solution to this is
30:27

called gradient clipping just trimming the gradient values to scale back bigger gradients into a smaller value
30:35
we can also have the opposite problem where now our weight values are very very small

30:41
and this leads to what is called the vanishing gradient problem and is also very problematic for training recurrent
30:47

neural models and we're going to touch briefly on three ways that we can mitigate this
30:53
vanishing gradient problem in recurrent models first choosing our choice of activation

30:59
function initially initializing the weights in our model intelligently and also designing our architecture of our
31:06

network to try to mitigate this issue altogether the reason why before we do that to take
31:13
a step back right the reason why vanishing gradients can be so problematic is that they can completely

31:20
sabotage this goal we have of trying to model long-term dependencies because we are multiplying many many
31:27

small numbers together what this effectively biases the model to do is to try to preferentially focus
31:34
on short-term dependencies and ignore the long-term dependencies that

31:39
may exist and while this may be okay for simple sentences like the clouds are in the
31:45

blank it really breaks down in longer sentences or longer sequences where
31:52
where information from earlier on in the sequence is very important for

31:57
making a prediction later on in the case of this example here
32:03

so how can we alleviate this our first strategy is a very simple
32:08
trick that we can employ when designing our networks we can choose our activation function

32:14
to prevent the gradient from shrinking too dramatically and the relu activation
32:20

function is a good choice for doing this because in instances where our input x
32:25
is greater than zero it automatically boosts the value of the activation function to

32:31
one whereas other activation functions don't do do that right
32:37

another trick is to be smart in how we actually initialize the parameters in our model
32:43
what we can do is initialize the weights that we set to the identity matrix which

32:49
prevents them from shrinking to zero too rapidly during back propagation
32:55

and the final and most robust solution is to use a more complex recurrent unit
33:01
that can effectively track long-term dependencies in the data and the idea here is that we're going to

33:07
introduce this computational infrastructure called the gate which functions to selectively add or
33:14

remove information to the state of the rnn and this is done
33:20
by you know standard operations that we see in neural networks for example sigmoid activation functions pointwise

33:28
matrix multiplications and the idea behind these gates is that they can effectively
33:34

control what information passes through the recurrent cell and today we're going to touch very briefly on one type of
33:41
gated cell called a lstm a long short term memory network and they're fairly good at using this

33:48
gating mechanism to selectively control information over many time steps
33:53

and so i'm not going to get into the details right because we have more interesting things to uh touch on in our
34:00
on our limited time but the key idea behind the lstms is they have that same

34:05
chain-like structure as a standard rnn but now the internal computation is a
34:10

little bit more complex right we have these different gates that are effectively interacting with each other
34:16
to try to control information flow and you would implement the lstm in tensorflow just as you would

34:24
a standard rnn and while that diagram i just like blew past that right the gated structure we
34:31

could spend some time talking about the mathematics of that but really what i want you to take away from this lecture
34:36
is the key concepts behind what the lstm is doing internally so to break that down the lstm like a

34:44
standard rnn is just maintaining this notion of self-state but it has these additional gates which
34:50

control the flow of information functioning to effectively eliminate irrelevant information from the past
34:58
keeping what's relevant keeping what's important from the current input

35:03
using that important information to update the internal cell state and then outputting a filtered version of that
35:09

cell state as the predictive output and what's key is that because we incorporate this great gated structure
35:17
in practice our backpropagation through time algorithm actually becomes much more stable and we can

35:24
mitigate against the vanishing gradient problem by having fewer repeated matrix multiplications that
35:30

allow for a smooth flow of gradients across our model okay
35:36
so now we've gone through the fundamentals of rnns in terms of their architecture and training and

35:42
i'd next like to consider a couple of concrete examples of how we can use recurrent role models
35:49

the first is let's imagine we're trying to use a recurrent model to predict the next
35:55
musical note in a sequence and use this to generate brand new musical sequences

36:01
what we can do is we can treat this as a next next input predict sorry a next
36:07

time step prediction problem where you input a sequence of notes and the output
36:13
at each time step is the most likely next note in the sequence and so for example

36:19
it turns out that this very famous classical composer named franz schubert
36:25

had what he called a very famous unfinished symphony and that was left
36:30
as the name suggests partially undone and he did not get a chance to actually finish composing the symphony before he

36:37
died and a few years ago some researchers trained a neural network model
36:43

on this uh on the prior movements of that symphony to try to actually generate new music
36:49
that would be similar to schubert's music to effectively finish the symphony and compose two new

36:55
movements so we can actually take a listen to what that result turned out
37:01

like
37:08
[Music]

37:18
so hopefully you you were able to hear that and and appreciate the point that maybe there are some classical music
37:24

aficionados out there who can recognize it as being stylistically similar hopefully to schubert's music
37:31
and you'll get practice with this exact same task in today's lab where you'll be training a recurrent model to generate

37:38
brand new irish folk songs that have never been heard before
37:44

as another cool example which i kind of motivated at the beginning of the lecture we can also do a classification task
37:52
where we take an input sequence and try to predict a single output associated

37:57
with that sequence for example taking a sequence of words and assigning an emotion or a sentiment associated with
38:04

that sequence and one use case for this type of task is in
38:10
tweet sentiment classification so training a model on a bunch of tweets from twitter and using it to predict

38:17
a sentiment associated with given tweets so for example
38:22

we can take we can train a model like this with a bunch of tweets hopefully we can train an rnn to predict that this first tweet
38:29
about our course has a positive sentiment but that this other tweet about winter weather is actually just

38:37
having a negative sentiment okay so at this point you know we focus
38:43

exclusively on recurrent models and it's actually fairly remarkable that with this type of architecture we can do
38:49
things that seem so complex as generating brand new classical music but let's take a step back right with

38:56
any technology they're going to be strengths and there are going to be limitations what could be potential issues of using
39:04

recurrent models to perform sequence modeling problems the first key limitation is that these
39:11
network architectures fundamentally have what we like to think of as an encoding bottleneck we need to take a lot of

39:18
content which may be a very long body of text many different words and condense it
39:23

into a representation that can be predicted on and information can be lost in this
39:29
actual encoding operation another big limitation is that recurrent

39:34
neurons and recurrent models are not efficient at all right they require sequentially processing information
39:41

taking time steps individually and this sequential nature makes them very very inefficient on the modern gpu
39:48
hardware that we use to train these types of models and finally and perhaps most importantly

39:55
while we've been emphasizing this point about long-term memory the fact is that
40:00

recurrent models don't really have that high of memory capacity to begin with
40:05
while they can handle sequences of length on the order of tens or even hundreds with lstms they don't

40:12
really scale well to sequences that are of length of thousands or ten thousands of time steps
40:18

how can we do better and how can we overcome this so to understand how to do this right
40:24
let's go back to what our general task is with sequence modeling we're given

40:30
some sequence of inputs we're trying to compute some sort of features associated with those inputs and use that to
40:37

generate some output prediction and with rnns as we saw we're using this
40:42
recurrence relation to try to model sequence dependencies but as i mentioned these rnns have these

40:50
three key bottlenecks what is the opposite of these these three limitations if we had any
40:56

capability that we desired what could we imagine the capabilities that we'd really like
41:03
to achieve with sequential models is to have a continuous stream of information that overcomes the encoding bottleneck

41:10
we'd like our model to be really fast to be paralyzable rather than being slow
41:15

and dependent on each of the individual time steps and finally we wanted to be able to have
41:21
long memory the main limitation of rnn's when it comes to these capabilities is that they

41:28
process these individual time steps individually due to the recurrence relation so what if we could eliminate
41:35

the recurrence relation entirely do away with it completely
41:40
one way we could do this is by simply taking our sequence and squashing everything together concatenating those

41:47
individual time steps such that we have one vector of input with data from all time points we can feed it into a model
41:55

calculate some feature vector and then generate an output which maybe we hope makes sense right
42:02
naively a first approach to do this may be to take that squashed concatenated

42:07
input pass it into a fully connected network and yay congrats we've eliminated the
42:13

need for recurrence but what are issues here right this is totally not scalable right our
42:20
dense network is very very densely connected right it has a lot of connections and our whole point

42:25
of doing this was to try to scale to very long sequences furthermore we've
42:31

completely eliminated any notion of order any notion of sequence and because of these two issues
42:37
our long memory that we want all along is also made impossible and so

42:43
this approach is definitely not going to work we don't have a notion of what points in our sequence is important
42:49

can we be more intelligent about this and this is really the key idea behind
42:55
the next concept that we're going to i'm going to introduce uh in the remaining time

43:00
and that's this notion of attention right which intuitively we're we're going to think about the ability to
43:07

identify and attend to parts of an input that are going to be important
43:13
and this idea of attention is an extremely powerful and rapidly emerging mechanism for modern neural networks

43:21
and the it it's the core foundational mechanism for this very very powerful
43:26

architecture called transformers you may have heard of transformers before in popular news media what have
43:34
you and the idea when you maybe want to try to look at the math and the operation of

43:39
transformers can seem very daunting it was definitely daunting for me as it tends to be presented in a pretty
43:46

complex and complicated way but at its core this attention mechanism which is the really key insight into
43:52
transformers is actually a very elegant and intuitive idea we're going to break it down step by

43:58
step so you can see how it's computed and what makes it so powerful to do that we're specifically going to
44:05

be talking about this idea of self-attention and what that means is the ability to take an input and attend
44:12
to the most important parts of that input i think it's easiest to build up that intuition by considering an image so

44:20
let's look at this picture of our hero iron man how do we figure out what's important
44:26

a naive way could just be to scan across this image pixel by pixel but as humans we don't do this our
44:33
brains are able to immediately pick up what is important in this image just by

44:38
looking at it that's iron man coming at us and if you think about it right what this comes down to is the ability to
44:45

identify the parts that are important to attend to and be able to extract features from those regions with high
44:52
attention and this first part of this problem is very very similar conceptually to

44:58
search and to build up understanding of this attention mechanism we're going to start
45:04

their first search how does search work so maybe sitting there listening to my lecture you're
45:10
thinking wow this is so exciting but i want to learn even more about neural networks how can i do this

45:16
one thing you could do is go to our friend the internet do a search and have all the videos on the internet
45:23

accessible to you and you want to find something that matches your desired goal
45:29
so let's consider youtube right youtube is a giant database of many many many videos

45:35
and across that database the ranging of different topics how can we find and attend to a relevant
45:41

video to what we're searching for right the first step you do is to input some
45:47
query some query into the youtube search bar your topic is deep learning

45:53
and what effectively can be done next is that for every video in this database we're going to extract some key
46:00

information which we call the key it's the title maybe associated with
46:05
that video and to do the search what can occur is the overlaps between your query

46:12
and the keys in that database are going to be computed and as we do this at each check we make
46:18

we'll ask how similar is that key the title of the video to our query deep
46:24
learning our first example right this video of a turtle is not that similar

46:29
our second example lecture from our course is similar and our third example kobe bryant is not
46:36

similar and so this is really this idea of computing what we'll come to see as an
46:43
attention mask measuring how similar each of these keys these video titles is to our query

46:50
our next and final step is to actually extract the information that we care about based on this computation
46:58

the video itself and we are going to call this the value and because our search was implemented
47:04
with a good notion of attention gratefully we've identified the best deep learning course out there for you

47:09
to watch and i'm sure all of you sitting there can hopefully relate and agree with with that assessment
47:17

okay so this concept of search is very very related to how self-attention works in neural networks like transformers
47:25
so let's go back from our youtube example to sequence modeling for example where we have now this sentence he

47:31
tossed the tennis ball to serve and we're going to break down step by step how self-attention will work over
47:38

the sequence first let's remember what we're trying to do we're trying to
47:44
identify and attend to the most important features in this input without any need for processing the information

47:51
time step by time step we're going to eliminate recurrence and what that means is that we need a
47:58

way to preserve order information without recurrence without processing the words in the sentence individually
48:05
the way we do this is by using an embedding that is going to incorporate some notion

48:12
of position and i'm going to touch on this very very briefly for the sake of time but the key idea is that you compute a
48:19

word embedding you take some metric that captures position information within that sequence you combine them together
48:26
and you get an embedding an encoding that has this notion of position baked in

48:31
you can we can talk about the math of this further if you like but this is the key intuition that i want you to come
48:38

away with our next step now that we have some notion of position from our input is to
48:45
actually figure out what in the input to attend to and that relates back to our

48:50
search operation that i'm motivated with the youtube example we're going to try to extract the query the key and the
48:57

value features and recall we're trying to learn a mechanism for self-attention
49:04
which means that we're going to operate on the input itself and only on the input itself

49:10
what we're going to do is we're going to create three new and unique
49:15

transformations of that embedding and these are going to correspond to our query our key and our value
49:22
all we do is we take our positional embedding we take a linear layer and do a matrix

49:27
multiplication that generates one output a query then we can make a copy of that same
49:33

positional embedding now we can take a separate and independent a different linear layer do the matrix
49:40
multiplication and get another transformation of that output that's our key and similarly do this also for the

49:47
value and so we have these three distinct transformations of that same positional
49:52

embedding our query our key and our value our next step right is to take these
49:58
three uh features right and actually compute this attention

50:03
weighting figuring out how much attention to pay to and where and this is effectively
50:10

thought of as the attention weighting and if you recall from our youtube example we focus on the similarity
50:17
between our query and our key and in neural networks we're going to do exactly the same right so if you recall

50:24
these query and key features are just numeric matrices or vectors
50:30

how can we compute their similarity their overlap so let's suppose we have two vectors
50:37
q and k and for vectors as you may recall from linear algebra or calculus we can

50:44
compute the similarity between these vectors using a dot product and then scaling that dot product
50:50

and this is going to quantify our similarity between our query and our key matrix vectors
50:56
this metric is also known as the cosine similarity and we can apply the same exact operation to matrices

51:05
where now we have a similarity matrix metric that captures the similarity
51:10

between the query matrix and the key matrix okay let's visualize what the result of this
51:17
operation could actually look like and mean remember we're trying to compute this self-attention we compute this dot

51:24
product of our query and our key matrices we apply our scaling and our last step is to apply a function called
51:31

the softmax which just squashes every value so that it falls between 0 and 1.
51:37
and what this results in is a matrix where now the entries reflect the relationship between the components of

51:44
our input to each other and so in this sentence example he tossed the tennis ball to serve
51:51

what you can see with this heat map visualization is that the words that are related to each other
51:57
tennis serve ball post have a higher weighting a higher attention

52:02
and so this matrix is what we call the attention weighting and it captures where in our input to
52:09

actually self attend to our final step is to use this weighting matrix to actually extract features with
52:16
high attention and we simply do this it's super elegant by taking that

52:21
attention weighting multiplying it by our value matrix and then getting a transformed version of of
52:28

what was our value matrix and this is our final output this reflects the features that
52:34
correspond to high attention okay that's it right i know that could be

52:40
fast so let's recap it as as sort of the last thing that we're going to touch on our goal identify and attend to the most
52:47

important features in the input how does this look like in an architecture right our first step was to
52:53
take this positional encoding copy three times right we then pass that into three separate

52:59
different linear layers computing the query the key and the value matrices
53:06

and then use these values multiply them together apply multiply them together
53:13
apply the scaling and apply the soft max to get this attention weighting matrix

53:19
and our final step was to take that attention weighting matrix apply it to
53:25

our value matrix and get this extraction of the features in our input that have
53:30
high attention and so it's these core operations that form this architecture shown on the right

53:36
which is called a self-attention head and we can just plug this into a larger network and it's a very very very
53:43

powerful mechanism for being able to attend to important features in an input
53:51
okay so that's it that's i know it's it's a lot to go through very quickly but hopefully you appreciate the intuition

53:58
of this attention mechanism and how it works final note that i'll make is that we can
54:04

do this multiple times right we can have multiple individual attention heads so in this example we're attending to iron
54:11
man himself but we can have independent attention heads that now pay attention to

54:16
different things in our input for example a background building or this little region shown on the far right
54:23

which is actually an alien spaceship coming at iron man from the back
54:29
all right this is the fundamentals of the attention mechanism and its application as i mentioned at

54:35
the beginning of this section has been most famous and most notable in these architectures called transformers
54:41

and they're very very very powerful architectures that have a variety of applications
54:47
most famously perhaps is in language processing so you may have seen these examples where there are these really

54:52
large language transformers that can do things like create images based on sentences for
55:00

example an armchair in the shape of an avocado and many other tasks ranging from
55:06
machine translation to dialogue completion and so on beyond language processing we can also

55:12
apply this mechanism of self-attention to other domains for example in biology
55:17

where one of the breakthroughs of last year was in protein structure structure prediction with a neural network
55:24
architecture called alpha fold 2. and a key component of alpha fold 2 is this exact same self-attention mechanism

55:32
and what what these authors showed was that this achieved really a breakthrough in the quality and accuracy of protein
55:39

structure prediction and a final example is that this this mechanism does not just apply
55:45
only to traditional sequence data we can also extend it to computer vision

55:50
with an architecture known as vision transformers the idea is very similar we
55:56

just need a way to encode positional information and then we can apply the attention mechanism to extract features
56:03
from these images in a very powerful and very high throughput manner

56:09
okay so hopefully you've gotten a sense over this course of this lecture about sequence modeling tasks and why
56:16

rnns as a introductory architecture are very powerful for processing sequential
56:21
data in that vein we discussed how we can model sequences using a recurrence

56:27
relation how we can train rnns using back propagation through time how we can apply rnns to different types
56:34

of tasks and finally we in this new component of the sequence modeling lecture we
56:40
discussed how we can move beyond recurrence and recurrent neural net networks to build self-attention

56:46
mechanisms that can effectively model sequences without the need for recurrence
56:52

okay so that concludes the two lectures for today and again i know we're we're running a bit late on
56:59
time and i apologize but i hope you enjoyed both of today's lectures

57:05
in the remaining hour that we have dedicated it will focus on the software lab sessions
57:10

a couple important notes for the software labs we're going to be running these labs in
57:15
a hybrid format you can find the information about downloading the labs on the course

57:21
website both the intro to deep learning course website as well as the canvas course website
57:27

all you will need to run the software labs is a computer internet and a google account
57:33
and you're going to basically walk through the labs and start executing the code blocks and fill out these to do

57:39
action items that will allow you to complete the labs and execute the code
57:44

and we will be holding office hours both virtually on gallertown
57:49
the link for that is on the course canvas page as well as in person in mit

57:54
room 10 250 for those of you who are on campus and would like to drop by for
57:59

in-person office hours alexander and myself will be there so
58:04
yeah with that i will i will conclude and thank you thank you again for your attention thank you for your patience

58:11
and hope you enjoyed it hope to see you in the software lab sessions thank you


